---
title: 'Dominican Real Estate Market'
output:
  html_document: 
    keep_md: yes
  pdf_document: default
  word_document: default
---

## Introduction

For this analysis, we will examine the effect that area (measured in squared meters) has on the value of apartment, for residential use, in Santo Domingo, Dominican Republic. Prices and apartment's characteristics are collect via web scraping (you can see [here](https://github.com/augustojimenez/residential_webscraping) how this was done). Specifically, data were retrieved on 22nd of march, 2022, from supercasas.com, a beacon on the online Dominican real estate market.

In this article, I will guide you step by step how I performed the data cleaning process and the reasons an particular step was chosen. Without further ado, let's begin.

## Loading libraries and data

First, I loaded the libraries used throughout the cleaning process, set the seed (so, results are reproducible), and cleared the objects from the work space. Finally, I loaded the data retrieved from supercasas.com on different dates.

```{r loading-libraries, message = FALSE}
rm(list = ls())
getwd()
options(scipen = 999)

library(robustbase)
library(tidyverse)
library(caret)

set.seed(1234)

path <- "../../0_Data Collection/1_data/0_raw/housing price/"
housing_files <- list.files(path)
housing <- read_csv(paste0(path, housing_files))
```

The data cleaning process involves seeing and slightly analyzing the data. As I will also perform the Exploratory Data Analysis and some statistical modelling in a later stage, I must not see and clean the whole dataset. Doing so might bias results.

I am interested in the data retrieved on March 22nd, 2022 for residential apartments listed in Santo Domingo. Here I filtered the dataset to match this criteria. Then, we split the dataset into training (70%) and testing (30%). I'll only use the training set to cleaning the dataset. In a later stage, this process will be applied to the test set.

```{r importing-data}
housing <- housing %>%
  filter(date ==  "2022-08-02",
         usage == "Residencial",
         province %in% c("Santo Domingo", "Santo Domingo Centro (D.N.)")) %>%
  rename(location = neighborhood) %>%
  select(-c(date, usage, city, province))

inTrain <- createDataPartition(housing$price.usd, p = 0.7, list = FALSE)

training <- housing[inTrain, ]
testing <- housing[-inTrain, ]
```

On the 22nd of March, 2022, there were 5,932 apartments for residential use listed in supercasas.com that were located in Santo Domingo. For creating the data cleaning process, I'll use a (randomly selected) subset of 4,154 apartments with the same overall characteristics.

```{r}
dim(housing)
dim(training)
dim(testing)
```

## Data cleaning

First, let's see what's on the dataset:

```{r}
glimpse(training)
```

Our training dataset contains `r nrow(training) %>% format(big.mark = ",")` observations and `r ncol(training) %>% format(big.mark = ",")` variables. Of them, we can highlight `price`, `currency` and `price.usd`. Where `price` and `currency` is the actual price shown in the listing's site. Any apartment can be listed in local currency or US dollars, depending on the seller's preference. I created `price.usd` on the data collection step to show prices on the same currency: US dollars. Hence, if prices were stated in local currency, they were converted into US dollars. Otherwise, they stay the same. I'll drop `currency` and `price`, and keep `price.usd`. Then, and for simplicity's sake, I'll rename `price.usd` as simply `price`.

```{r}
training <- training %>%
  select(-c(currency, price)) %>%
  rename(price = price.usd)
```

When looking at the proportion of `NA`s are present per variable, over 50% of listings did not provide information regarding the floor the apartment is located at. And so, I removed that variable. The proportion of missing values for all other variables is acceptable.

```{r}
apply(training, 2, 
      \(x) {
        n <- length(x)
        na <- x %>%
          is.na() %>%
          sum()
        prop.na <- na / n * 100
        })

training <- training %>%
  select(-c(story))
```

Before implementing some formal procedure for removing outliers, let's analyse the data at hand. First, we can see that `area` has some outstanding observations: a minimum value of `r min(training$area, na.rm = TRUE) %>% round(2) %>% format(big.mark = ",")` and a maximum of `r max(training$area, na.rm = TRUE) %>% round(2) %>% format(big.mark = ",")` squared meters.

```{r}
summary(training)
```

By viewing the "largest apartments", some things become apparent.

1.  First, there are repeated observations on this small sample: see the third and fourth rows, for instance.

2.  Second, the first four rows are obviously typos: the seller typed 650,000 instead of 650 squared meters, to cited the first case.

3.  Third, the apartment listed on the fifth row is no longer available rising some doubts on its veracity.

4.  Last, the sixth "apartment" is actually a house. When analyzing the "smallest apartments", everything seems in order.

```{r}
training %>%
  arrange(desc(area)) %>%
  head(10)
```

So, to fix (1) I eliminated duplicates. To solve (2), I divided by 1,000 the area of those apartments with over 10,000 squared meters of area. Finally, as for (3), I removed those apartments that are obviously not of interest.

```{r}
training <- training %>%
  filter(id != "/apartamentos-venta-cuesta-hermosa-ii/1236477/",
         id != "/apartamentos-venta-los-cacicazgos/1272251/") %>%
  mutate(area = ifelse(area > 10000, area / 1000, area)) %>%
  unique()
```

Let's do the same with `price`. But viewing `price` or `area` alone might be misleading as an apartment with 30 squared meters could be worth 20,000 dollars, but one with 280 squared meters could hardly be worth \$10,256. What I mean is, price must be analyse in regards with area. Price per squared meter could tell us more about how extreme of a value it is.

```{r}
summary(training$price)
ggplot(training, aes(price)) +
  geom_histogram()

```

```{r}
summary(training$area)
ggplot(training, aes(area)) +
  geom_histogram()
```

## Some feature engineering

When analysed individually, `price` and `area` show some pretty hardcore outliers. What about price in relation to area, that's price per squared meter, or area in relation to the number of bedrooms? Let's do some feature engineering to create measures for these relations:

```{r}
training <- training %>%
  mutate(price_per_m2 = price / area,
         area_per_br = area / bedrooms)
```

Well, I still got similar results ...

```{r}
summary(training$price_per_m2)

ggplot(training, aes(price_per_m2)) +
  geom_histogram()
```

It's still rather blurry... I cannot see a clear pattern in the data distribution. Let's filter data below the third quartile, based on price per squared meter.

```{r}
training %>%
  filter(price_per_m2 < 1864) %>%
  ggplot(aes(price)) +
  geom_histogram()
```

## Removing outliers

So far, I have removed some obvious outliers and determined the distribution of price per squared meter (right skewed). Knowing that, I can pick a suitable method for detecting outliers. For right skewed distributions, I cannot simply use Z scores (suitable for normal distributions).

I think that anything below \$200 per squared meter seems rather dubious, don't you think? But that's very subjective, and we don't want that, do we? Let's filter them out. But let's use something less subjective. The library `robustbase` contains some useful functions for basic robust statistics. It includes, among other things, an implementation of the methods proposed by Hubert and Vandervieren (2008) for outlier detection for skewed distributions. Using this, I got these cutoff values for `price_per_m2`:

```{r}
(cutoff <- adjboxStats(training$price_per_m2)$fence)
```

When I filtered using these new cutoff values, the distribution looks a lot better:

```{r}
training <- training %>%
  filter(between(price_per_m2, cutoff[1], cutoff[2]))

summary(training$price_per_m2)
ggplot(training, aes(price)) +
  geom_histogram()
```

By the way, when I analysed a sample of the apartments tagged as outliers, it turned out that they were typos. Either they were listed in the wrong currency or it missed some zeros.

I used the same method to remove \`area_per_br\` outliers:

```{r}
summary(training$area_per_br)

(cutoff <- adjboxStats(training$area_per_br)$fence)

training <- training %>%
  filter(between(area_per_br, cutoff[1], cutoff[2]))

summary(training$area_per_br)
```

Many times, the same apartment is listed by different sellers. So, eliminating duplicates is not enough to remove confliting. Some seller are not as rigourous as to list all the amaneties, so we are going to keep the most complete listing:

## Some more feature engineering

`status` refers to the current status of the apartment listed. It should be an ordered categorical variable as it goes through several stages from blueprint up to used:

```{r}
training <- training %>%
  mutate(status = case_when(status == "En Planos" ~ "Building",
                            status == "En Construcci√≥n" ~ "Building",
                            status == "Nueva" ~ "New",
                            status == "Remodelada" ~ "Rebuilt",
                            status == "A Remodelar" ~ "Rebuilding",
                            status == "Segundo Uso" ~ "Used",
                            status == "Fideicomiso" ~ "Trust",
                            TRUE ~ status),
         status = factor(status, levels = c("In blueprint", "Building",
                                            "New", "Rebuilt", "Rebuilding",
                                            "Used", "Other")))
```

There's an exception for this: Trust, and it refers to the way the building company is structured. Meaning that the building company is building the apartments through a trust fund. But the actual stage is unknown. Let's see how many are there.

```{r}
training[training$status == "Trust" & !is.na(training$status), ]
```

There are only two listings with this status. Let's remove them.

```{r}
training <- training %>%
  filter(status != "Trust")
```

Our final data frame has 20 columns wide and 2,851 rows long. I'll store it in a new location with a new name.

```{r}
glimpse(training)
write_csv(training, "../1_data/training_set.csv")
```
